{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aa0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062b6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.loader import load_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5615e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunks = load_docs(\"app/data/prince_rag_knowledge_base.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d42338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT ME – PRINCE KUMAR\n",
      "\n",
      "Prince Kumar is a Full Stack Python Developer and AI/ML Engineer currently pursuing a B.Tech in Mechanical Engineering at NIT Jalandhar (2023–2027).\n",
      "He has strong interest in Artificial Intelligence, Machine Learning, Deep Learning, Computer Vision, NLP, and Full Stack Development.\n"
     ]
    }
   ],
   "source": [
    "print(data_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c961482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_key: pcsk_e5cfc_1HBPr3Wuk5W6AfJuqgz1BiTBvjA9biAGmRU5rpkCLesX6LiyomGNNo5QgfyGZw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "print(\"api_key:\", api_key)\n",
    "\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5737e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b14842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone_index_name: portdolio-info\n",
      "\u001b[93mIndex created with dimension 384 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load index name\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "print(\"pinecone_index_name:\", index_name)\n",
    "\n",
    "# Delete existing index if it exists to recreate with correct dimension\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "    print(\"\\033[93mExisting index deleted.\\033[0m\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=EMBEDDING_DIM,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "    )\n",
    ")\n",
    "print(\"\\033[93mIndex created with dimension\", EMBEDDING_DIM, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ee5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8955ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(\n",
    "#     model=\"models/embedding-001\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5582b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load a free sentence-transformers model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "395ed1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fbcf4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Get embedding dimension\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "EMBEDDING_DIM = len(test_embedding)\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f576048",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6728db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "# Create vector store\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=data_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ae3176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for query: give me linkin id\n",
      "\n",
      "Result 1:\n",
      "LEADERSHIP\n",
      "\n",
      "- Core Member, Deja Vu Club, NIT Jalandhar (2025–2026)\n",
      "\n",
      "ONLINE PRESENCE\n",
      "\n",
      "GitHub: https://github.com/princekumar983777\n",
      "LinkedIn: https://www.linkedin.com/in/prince-kumar-021460290\n",
      "\n",
      "Result 2:\n",
      "Deep Learning and Computer Vision\n",
      "- Develop CNN-based models\n",
      "- Implement face recognition and emotion detection systems\n",
      "- Build real-time computer vision applications using OpenCV\n",
      "\n",
      "NLP and RAG Systems\n",
      "- Build RAG-based chatbots\n",
      "- Implement semantic search and vector embeddings\n",
      "- Use Hugging Face Transformers for NLP tasks\n",
      "- Design domain-specific AI assistants\n",
      "\n",
      "Result 3:\n",
      "INTERNSHIP\n",
      "\n",
      "AI/ML Intern – AD Infocom Systems, Nagpur (Remote)\n",
      "June 2025 – July 2025\n",
      "- Worked with real-world datasets\n",
      "- Implemented ML algorithms\n",
      "- Built and tested ML models using Python\n",
      "\n",
      "ACHIEVEMENTS\n",
      "\n",
      "- 1st rank in college-level hackathon\n",
      "- Built 10+ ML mini-projects\n",
      "- Diamond Rank on Google Skill Boost\n",
      "- Solved 200+ DSA problems\n",
      "- Participated in multiple hackathons\n",
      "\n",
      "LEADERSHIP\n",
      "\n",
      "- Core Member, Deja Vu Club, NIT Jalandhar (2025–2026)\n",
      "\n",
      "ONLINE PRESENCE\n"
     ]
    }
   ],
   "source": [
    "# Example search in the vector store\n",
    "query = \"give me linkin id\"\n",
    "results = vectorstore.similarity_search(query, k=3)  # k=3 means top 3 results\n",
    "\n",
    "print(\"Search results for query:\", query)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1cd316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform semantic search and generate response with Gemini\n",
    "def search_and_respond(query):\n",
    "    # Step 1: Search the vector store for relevant documents\n",
    "    search_results = vectorstore.similarity_search(query, k=3)  # Get top 3 relevant chunks\n",
    "\n",
    "    # Step 2: Extract the content from search results\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in search_results])\n",
    "\n",
    "    # Step 3: Create a prompt that includes the context\n",
    "    prompt = f\"\"\"Use the following context to answer the question. If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 4: Use Gemini to generate the response\n",
    "    client = genai.Client()\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        generation_config={\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_output_tokens\": 512,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.text, search_results\n",
    "\n",
    "# Example usage\n",
    "query = \"What are Prince Kumar's skills?\"\n",
    "response, sources = search_and_respond(query)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)\n",
    "print(\"\\nSources used:\")\n",
    "for i, doc in enumerate(sources):\n",
    "    print(f\"{i+1}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e32252",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"How does AI work?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdf92dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, how lovely! What kind of dogs are they?\n",
      "Assuming both your dogs have all their paws, that would be:\n",
      "\n",
      "2 dogs * 4 paws/dog = **8 paws** in your house!\n",
      "role - user: I have 2 dogs in my house.\n",
      "role - model: Oh, how lovely! What kind of dogs are they?\n",
      "role - user: How many paws are in my house?\n",
      "role - model: Assuming both your dogs have all their paws, that would be:\n",
      "\n",
      "2 dogs * 4 paws/dog = **8 paws** in your house!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
    "\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)\n",
    "\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a8a2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT ME – PRINCE KUMAR\n",
      "\n",
      "Prince Kumar is a Full Stack Python Developer and AI/ML Engineer currently pursuing a B.Tech in Mechanical Engineering at NIT Jalandhar (2023–2027).\n",
      "He has strong interest in Artificial Intelligence, Machine Learning, Deep Learning, Computer Vision, NLP, and Full Stack Development.\n",
      "\n",
      "He focuses on building real-world AI-powered applications by combining backend systems, APIs, databases, and modern AI models.\n",
      "He emphasizes practical implementation, clean architecture, and scalable solutions.\n",
      "\n",
      "PROFESSIONAL OBJECTIVE\n",
      "\n",
      "Prince aims to work on AI-driven software systems that solve real-world problems using Python, Machine Learning, Deep Learning, and Full Stack technologies.\n",
      "He is actively seeking internships, freelance projects, and collaborative opportunities.\n",
      "\n",
      "CORE EXPERTISE\n",
      "\n",
      "- Full Stack Python Development\n",
      "- Machine Learning and Data Science\n",
      "- Deep Learning and Computer Vision\n",
      "- NLP and Retrieval-Augmented Generation (RAG)\n",
      "- Backend APIs and System Design\n",
      "- Databases and Data Analytics\n",
      "- AI-powered Web Applications\n",
      "\n",
      "WHAT I CAN DO\n",
      "\n",
      "Full Stack Development\n",
      "- Build end-to-end web applications using Django, Flask, and FastAPI\n",
      "- Design REST APIs with authentication and authorization\n",
      "- Integrate frontend using HTML, CSS, JavaScript, and Tailwind CSS\n",
      "- Develop scalable backend systems\n",
      "\n",
      "AI and Machine Learning\n",
      "- Data cleaning, preprocessing, and feature engineering\n",
      "- Train supervised and unsupervised ML models\n",
      "- Model evaluation and optimization\n",
      "- Build ML pipelines using Python\n",
      "\n",
      "Deep Learning and Computer Vision\n",
      "- Develop CNN-based models\n",
      "- Implement face recognition and emotion detection systems\n",
      "- Build real-time computer vision applications using OpenCV\n",
      "\n",
      "NLP and RAG Systems\n",
      "- Build RAG-based chatbots\n",
      "- Implement semantic search and vector embeddings\n",
      "- Use Hugging Face Transformers for NLP tasks\n",
      "- Design domain-specific AI assistants\n",
      "\n",
      "Data Analytics\n",
      "- Perform Exploratory Data Analysis (EDA)\n",
      "- Create dashboards using Power BI and Excel\n",
      "- Visualize insights using Matplotlib and Seaborn\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "EDA on Multiple Datasets (2024)\n",
      "- Worked on 10+ datasets across finance, healthcare, and social media\n",
      "- Data cleaning, feature engineering, and visualization\n",
      "- Extracted actionable insights\n",
      "\n",
      "RAG-Based Medical Chatbot (2025)\n",
      "- Built a medical chatbot using Retrieval-Augmented Generation\n",
      "- Integrated knowledge base for context-aware responses\n",
      "- Implemented semantic retrieval and LLM-based answering\n",
      "\n",
      "Face Recognition and Emotion Detection (2025)\n",
      "- Developed real-time face recognition and emotion detection system\n",
      "- Built Streamlit app for live webcam predictions\n",
      "\n",
      "TECHNICAL SKILLS\n",
      "\n",
      "Programming: Python, C, C++\n",
      "Machine Learning: NumPy, Pandas, Scikit-learn\n",
      "Deep Learning: TensorFlow, PyTorch, CNNs, OpenCV\n",
      "NLP: Hugging Face, RAG Models, Tokenization\n",
      "Full Stack: Django, Flask, FastAPI, REST APIs\n",
      "Frontend: HTML, CSS, JavaScript, Tailwind CSS\n",
      "Databases: MySQL, PostgreSQL, MongoDB\n",
      "Analytics: Excel, Power BI, Data Visualization\n",
      "Tools: GitHub, VS Code, Jupyter, Colab, Kaggle, MLOps basics\n",
      "\n",
      "INTERNSHIP\n",
      "\n",
      "AI/ML Intern – AD Infocom Systems, Nagpur (Remote)\n",
      "June 2025 – July 2025\n",
      "- Worked with real-world datasets\n",
      "- Implemented ML algorithms\n",
      "- Built and tested ML models using Python\n",
      "\n",
      "ACHIEVEMENTS\n",
      "\n",
      "- 1st rank in college-level hackathon\n",
      "- Built 10+ ML mini-projects\n",
      "- Diamond Rank on Google Skill Boost\n",
      "- Solved 200+ DSA problems\n",
      "- Participated in multiple hackathons\n",
      "\n",
      "LEADERSHIP\n",
      "\n",
      "- Core Member, Deja Vu Club, NIT Jalandhar (2025–2026)\n",
      "\n",
      "ONLINE PRESENCE\n",
      "\n",
      "GitHub: https://github.com/princekumar983777\n",
      "LinkedIn: https://www.linkedin.com/in/prince-kumar-021460290\n"
     ]
    }
   ],
   "source": [
    "file = open(\"app/data/prince_rag_knowledge_base.txt\", \"r\", encoding=\"utf-8\")\n",
    "knowledge_base = file.read()\n",
    "print(knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b2ebdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Chats.create() got an unexpected keyword argument 'system_instruction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m client = genai.Client()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m chat = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are Prince Kumar. A backend developer and AI enthusiast. Use the below knowledge base to answer the questions.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m response = chat.send_message(\u001b[33m\"\u001b[39m\u001b[33mtell me about yourself.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[31mTypeError\u001b[39m: Chats.create() got an unexpected keyword argument 'system_instruction'"
     ]
    }
   ],
   "source": [
    "\n",
    "client = genai.Client()\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    system_instruction=\"You are Prince Kumar. A backend developer and AI enthusiast. Use the below knowledge base to answer the questions.\\n\\n\" + knowledge_base,\n",
    "    generation_config={\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_output_tokens\": 256,\n",
    "    }\n",
    ")\n",
    "\n",
    "response = chat.send_message(\"tell me about yourself.\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cbe8d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (7.3.0)\n",
      "Collecting pinecone\n",
      "  Using cached pinecone-8.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone) (2025.7.14)\n",
      "Requirement already satisfied: orjson>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone) (3.11.5)\n",
      "Collecting pinecone-plugin-assistant<4.0.0,>=3.0.1 (from pinecone)\n",
      "  Using cached pinecone_plugin_assistant-3.0.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.1.0,>=0.0.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone) (4.14.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Using cached pinecone-8.0.0-py3-none-any.whl (745 kB)\n",
      "Using cached pinecone_plugin_assistant-3.0.1-py3-none-any.whl (280 kB)\n",
      "Installing collected packages: pinecone-plugin-assistant, pinecone\n",
      "\n",
      "  Attempting uninstall: pinecone-plugin-assistant\n",
      "\n",
      "    Found existing installation: pinecone-plugin-assistant 1.8.0\n",
      "\n",
      "    Uninstalling pinecone-plugin-assistant-1.8.0:\n",
      "\n",
      "      Successfully uninstalled pinecone-plugin-assistant-1.8.0\n",
      "\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "  Attempting uninstall: pinecone\n",
      "   ---------------------------------------- 0/2 [pinecone-plugin-assistant]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "    Found existing installation: pinecone 7.3.0\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "    Uninstalling pinecone-7.3.0:\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "      Successfully uninstalled pinecone-7.3.0\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   -------------------- ------------------- 1/2 [pinecone]\n",
      "   ---------------------------------------- 2/2 [pinecone]\n",
      "\n",
      "Successfully installed pinecone-8.0.0 pinecone-plugin-assistant-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-pinecone 0.2.13 requires pinecone[asyncio]<8.0.0,>=6.0.0, but you have pinecone 8.0.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pinecone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
